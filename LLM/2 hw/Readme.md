# Домашняя работа по LLM: оценка двух моделей на задаче генеративной классификации (sentiment / emotion)

## Цель задания
Найти и выбрать какую-либо задачу (поискать на hugginface с тегом Russian или любым другим образом) и оценить с помощью нее **2 модели** на выбор.  
- Важно выбрать ту задачу, которую можно оценить на основе
**генерации LLM**
- Например, NER, где модель обязана генерировать ответ в
структурированном виде, который легко парсить

## Выбранные модели
- **Lamapi/next-1b**: https://huggingface.co/Lamapi/next-1b  
- **Unbabel/Tower-Plus-2B**: https://huggingface.co/Unbabel/Tower-Plus-2B  

## Попытка 1: NER (неудачно)
Сначала была выбрана NER-задача (token classification), где нужно присвоить метку *каждому токену/слову* в предложении.
Датасет: **belyakoff/NLI-NER-example** - https://huggingface.co/datasets/belyakoff/NLI-NER-example (в карточке датасета указаны язык Russian и формат parquet).  
На практике модели почти не давали корректного структурированного ответа для полного совпадения по словам, поэтому качество оказалось крайне низким (precision ≈ 0.025), после чего было решено выбрать более простую генеративно-оцениваемую задачу.

## Итоговая задача: определение эмоции текста
Был выбран датасет **Aniemore/cedr-m7** (классы эмоций/настроений) - https://huggingface.co/datasets/Aniemore/cedr-m7.  
Постановка для LLM: по входному тексту модель должна сгенерировать **ровно одну метку класса** (например, `neutral`, `sadness`, `happiness` и т.д.), что удобно парсится и сравнивается с ground truth.

## Метрика и протокол
Оценивание проводилось как задача многоклассовой классификации по сгенерированной метке:
- Precision / Recall / F1-score по каждому классу.
- Micro avg / Macro avg / Weighted avg по всему тесту.
- Размер выборки: **500** примеров (Support суммарно 500).

## Результаты

### Lamapi/next-1b
| Class | Precision | Recall | F1-Score | Support |
|---|---:|---:|---:|---:|
| neutral | 0.667 | 0.009 | 0.017 | 226 |
| fear | 0.812 | 0.703 | 0.754 | 37 |
| enthusiasm | 0.087 | 0.344 | 0.139 | 32 |
| sadness | 0.750 | 0.400 | 0.522 | 105 |
| happiness | 0.375 | 0.481 | 0.422 | 81 |
| anger | 0.128 | 0.882 | 0.224 | 17 |
| disgust | 0.000 | 0.000 | 0.000 | 2 |
| **micro avg** | **0.291** | **0.270** | **0.280** | **500** |
| **macro avg** | **0.403** | **0.403** | **0.297** | **500** |
| **weighted avg** | **0.590** | **0.270** | **0.258** | **500** |

Наблюдение: модель нестабильна - высокий Precision у одних классов (например, `neutral`, `sadness`) и высокий Recall у других (например, `anger`), но это не конвертируется в высокий F1.

### Unbabel/Tower-Plus-2B
| Class | Precision | Recall | F1-Score | Support |
|---|---:|---:|---:|---:|
| neutral | 0.752 | 0.482 | 0.588 | 226 |
| fear | 0.337 | 0.919 | 0.493 | 37 |
| enthusiasm | 0.098 | 0.156 | 0.120 | 32 |
| sadness | 0.892 | 0.629 | 0.737 | 105 |
| happiness | 0.833 | 0.556 | 0.667 | 81 |
| anger | 0.192 | 0.824 | 0.311 | 17 |
| disgust | 0.000 | 0.000 | 0.000 | 2 |
| **micro avg** | **0.548** | **0.546** | **0.547** | **500** |
| **macro avg** | **0.443** | **0.509** | **0.417** | **500** |
| **weighted avg** | **0.700** | **0.546** | **0.583** | **500** |

Наблюдение: Tower-Plus-2B более сбалансирована и предсказуема - стабильно лучше по метрикам на крупных классах и заметно повышает итоговые micro/weighted F1.

## Вывод
- Для выбранной генеративной постановки на **Aniemore/cedr-m7** модель **Unbabel/Tower-Plus-2B** показала существенно более высокие агрегированные метрики (micro F1 0.547 vs 0.280; weighted F1 0.583 vs 0.258) и более стабильное поведение по классам.  
- **Lamapi/next-1b** может быть "перекошена" по классам: хороша на отдельных эмоциях, но плохо удерживает баланс precision/recall, что ухудшает итоговый F1.