# LLM: общее описание работ
В рамках серии домашних работ были последовательно выполнены задания по ключевым практикам работы с LLM: запуск и сравнение моделей, выбор и оценка задачи на датасете, построение RAG-системы и fine-tuning с LoRA.
## ДЗ1 - Сравнение LLM (ручная оценка)
Собран ноутбук для инференса и проведено сравнение **Zephyr-7B-Alpha** и **Mistral-7B-Instruct-v0.2** на 10 не самых простых промптах (локально на RTX 3070 Ti). По наблюдениям: Zephyr чаще давала более развёрнутые ответы, но хуже держала русский и допускала фактические ошибки; Mistral лучше подхватывала русский, но местами была менее стабильной (артефакты/обрывы).

## ДЗ2 - Оценка на датасете (автоматические метрики)
Сначала была попытка NER, но генеративная постановка оказалась слишком строгой (почти нулевая точность). Затем выбрана классификация эмоций **Aniemore/cedr-m7** и сравнены **Lamapi/next-1b** и **Unbabel/Tower-Plus-2B** по Precision/Recall/F1: Tower-Plus-2B показала более сбалансированные и устойчивые результаты.

## ДЗ3 - RAG (Qdrant + LangChain)
Построена RAG-система: корпус из **rus_xquadqa** проиндексирован в Qdrant (:memory:), для поиска использованы эмбеддинги **multilingual-e5-large**, далее найденный контекст подаётся в LLM. На 10 вопросах точность выросла с 2/10 (без контекста) до 7/10 (с RAG), снизилось число галлюцинаций.

## ДЗ4 - Fine-tuning с LoRA
Выполнен LoRA fine-tuning на задаче из ДЗ2 (Aniemore/cedr-m7): 1000 примеров train и 100 test. Качество на тесте выросло с 37% до 51%, что показало пользу параметро-эффективной адаптации под конкретную задачу.